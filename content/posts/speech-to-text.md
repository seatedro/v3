---
date: 2022-09-17
author:
  name: Rohit Prakash
summary: Realtime speech-to-text
tags:
  - webrtc
  - js
  - gcp
title: Speech To Text in Realtime
---

![speech](/static/images/soundwave.webp)
So you want to transcribe audio from your microphone in real-time with a readily available API with a good response time?

---

### Setting up the Server

```bash
mkdir server && cd server
npm init -y
npm i -D typescript @tsconfig/node16 concurrently @types/pumpify @types/node @types/express
npx tsc --init
npm i socket.io express
```

Next, we open the `tsconfig.json` file generated by the above command. Make sure it look something like this:

```json
{   
    "extends": "@tsconfig/node16/tsconfig.json",
    "compilerOptions": 
    {     
        "target": "es5",
        "module": "commonjs",
        "outDir": "dist",
        "strict": true,
        "esModuleInterop": true,
        "skipLibCheck": true,
        "forceConsistentCasingInFileNames": true
     },
    "include": ["src"],
    "exclude": ["node_modules"]
}
```
Let’s set up TypeScript to watch for code changes and transpile it to JavaScript on the fly with `concurrently`.

Add the following script to your `package.json`
```json
"scripts": {
  "dev": "concurrently  \"npx tsc --watch\" \"nodemon -q ./dist/index.js\"
}
```

Now to start the server, simply run `npm run dev`{lang="zsh"}.

---

### Coding the `server.ts` file

This is a basic express server boilerplate you can use to get up and running.
```ts
import express, { Express, Request, Response } from "express";
import { createServer } from "https";
const app: Express = express();
const server = createServer(
  {
    key: fs.readFileSync("key.pem"),
    cert: fs.readFileSync("cert.pem"),
  },
  app
);
const port = 3000;
const io = new Server(server, {
  cors: {
    origin: "*",
  },
});

server.listen(port, () => {
  console.log(`⚡️[server]: Server is running at https://localhost:${port}`);
});
```

Next step, is to set up `socket.io` and add some `eventListeners`.

```ts
import { google } from "@google-cloud/speech/build/protos/protos";
import { Server, Socket } from "socket.io";
import speechToTextUtils from "./speechToTextUtils";

io.on("connection", (socket: Socket) => {
  console.log("Socket Connection: ", socket.connected);
  console.log("Socket Id: ", socket.id);
  speechToTextUtils._socket = socket;
  
  // Define socket eventListeners.
  socket.on(
    "startGoogleCloudStream",
    (request: google.cloud.speech.v1.IStreamingRecognitionConfig) => {
      // Very long type, but alas that's what Google decided to use.
      speechToTextUtils._request = request;
      console.log("Starting Google Cloud Transcription");
      speechToTextUtils.startRecognitionStream();
    }
  );
  
  // Receive audio data from front-end
  socket.on("binaryAudioData", (data: DataView) => {
    speechToTextUtils.receiveData(data);
  });

  // End the audio stream
  socket.on("endGoogleCloudStream", () => {
    speechToTextUtils.stopRecognitionStream();
  });
});
```

Let’s go over what the `eventListeners` do here:

* `startGoogleCloudStream`: As the name says, this initializes the `speechToTextUtils` class and sets up the options necessary which we will look at soon.

* `binaryAudioData`: This receives the audio data from the front-end as binary int16 buffers. This is processed by the `receiveData` function in `speechToTextUtils`.

* `endGoogleCloudStream`: This calls a helper function that turns off all eventListeners and shuts down the transcription.

Now, let's look at the `speechToTextUtils.ts` file.

```ts
import speech, { SpeechClient } from '@google-cloud/speech';
import { google } from '@google-cloud/speech/build/protos/protos';
import * as pumpify from 'pumpify';
import chalk from 'chalk';
import { Socket } from 'socket.io';
let speechClient: SpeechClient | null = null;

class SpeechToTextUtils {
	recognizeStream!: pumpify | null;
	resultEndTime = 0;
	isFinalEndTime = 0;
	finalRequestEndTime = 0;
	bridgingOffset = 0;
	streamingLimit = 290000;
	restartCounter = 0;
	lastTranscriptWasFinal = false;
	audioInput: DataView[] = [];
	lastAudioInput: DataView[] = [];
	newStream = true;
	socket!: Socket;
	request!: google.cloud.speech.v1.IStreamingRecognitionConfig | undefined;
	restartTimeout: NodeJS.Timeout | undefined;

	set _socket(value: Socket) {
		this.socket = value;
	}

	set _request(value: google.cloud.speech.v1.IStreamingRecognitionConfig) {
		this.request = value;
	}

	startRecognitionStream() {
		this.audioInput = [];
		if (!speechClient) {
			speechClient = new speech.SpeechClient(); // Creates a client
		}
		this.recognizeStream = speechClient
			.streamingRecognize(this.request)
			.on('error', (err) => {
				console.error('Error when processing audio: ' + err);
				this.socket.emit('googleCloudStreamError', err);
				this.stopRecognitionStream();
			})
			.on('data', this.speechCallback.bind(this));

		this.restartTimeout = setTimeout(
			this.restartStream.bind(this),
			this.streamingLimit
		);
	}

	speechCallback(stream: google.cloud.speech.v1.StreamingRecognizeResponse) {
		// Null checks
		if (
			stream.results &&
			stream.results[0] &&
			stream.results[0].resultEndTime &&
			stream.results[0].resultEndTime.nanos &&
			stream.results[0].resultEndTime.seconds &&
			stream.results[0].alternatives &&
			stream.results[0].isFinal
		) {
			// Convert API result end time from seconds + nanoseconds to milliseconds
			// The below seconds are useful to see the timestamps in the console
			let seconds: number;
			if (typeof stream.results[0].resultEndTime.seconds === 'string')
				seconds = parseInt(stream.results[0].resultEndTime.seconds);
			else if (Long.isLong(stream.results[0].resultEndTime.seconds))
				seconds = stream.results[0].resultEndTime.seconds.toNumber();
			else seconds = stream.results[0].resultEndTime.seconds;
			this.resultEndTime =
				seconds * 1000 +
				Math.round(stream.results[0].resultEndTime.nanos / 1000000);

			// Calculate correct time based on offset from audio sent twice
			const correctedTime =
				this.resultEndTime -
				this.bridgingOffset +
				this.streamingLimit * this.restartCounter;

			process.stdout.clearLine(0);
			process.stdout.cursorTo(0);
			let stdoutText = '';
			if (stream.results[0] && stream.results[0].alternatives[0]) {
				stdoutText =
					correctedTime + ': ' + stream.results[0].alternatives[0].transcript;
			}

			if (stream.results[0].isFinal) {
				process.stdout.write(chalk.green(`${stdoutText}\n`));
				this.socket.emit(
					'speechData',
					stream.results[0].alternatives[0].transcript
				);

				this.isFinalEndTime = this.resultEndTime;
				this.lastTranscriptWasFinal = true;
			} else {
				// Make sure transcript does not exceed console character length
				if (stdoutText.length > process.stdout.columns) {
					stdoutText =
						stdoutText.substring(0, process.stdout.columns - 4) + '...';
				}
				process.stdout.write(chalk.red(`${stdoutText}`));

				this.lastTranscriptWasFinal = false;
			}
		}
	}
}

export default new SpeechToTextUtils();
```

We have a couple of setters which help us retain the socket instance and the request config.

The `startRecognitionStream` method initializes the cloud speech client with the request config and binds the `speechCallback` which is triggered everytime the Google API returns a transcription response.

We use `chalk` to highlight the timestamps and the transcriptions live in red, and green once it is final.

The restartTimeout triggers the restartStream method once the timer has reached about 5 minutes as this is the max continuous recognition limit Google has set for the Speech-To-Text API.

How is the input audio handled? We’ll look at that next.

```ts
/**
   * Receives streaming data and writes it to the recognizeStream for transcription
   *
   * @param {Buffer} data A section of audio data
   */
  receiveData(data: DataView) {
    if (
      this.newStream &&
      this.lastAudioInput.length !== 0 &&
      this.recognizeStream
    ) {
      // Approximate math to calculate time of chunks
      const chunkTime = this.streamingLimit / this.lastAudioInput.length;
      if (chunkTime !== 0) {
        if (this.bridgingOffset < 0) {
          this.bridgingOffset = 0;
        }
        if (this.bridgingOffset > this.finalRequestEndTime) {
          this.bridgingOffset = this.finalRequestEndTime;
        }
        const chunksFromMS = Math.floor(
          (this.finalRequestEndTime - this.bridgingOffset) / chunkTime
        );
        this.bridgingOffset = Math.floor(
          (this.lastAudioInput.length - chunksFromMS) * chunkTime
        );

        for (let i = chunksFromMS; i < this.lastAudioInput.length; i++) {
          this.recognizeStream.write(this.lastAudioInput[i]);
        }
      }
      this.newStream = false;
    }

    this.audioInput.push(data);

    if (this.recognizeStream) {
      this.recognizeStream.write(data);
    }
  }
```

This function `receiveData` is from Google’s node client library [Github Repo](https://github.com/googleapis/nodejs-speech/tree/main). It writes the binary chunks of audio data received into `streamingRecognize`.

```ts
  restartStream() {
    if (this.recognizeStream) {
      this.recognizeStream.end();
      this.recognizeStream.removeAllListeners();
      this.recognizeStream = null;
    }
    if (this.resultEndTime > 0) {
      this.finalRequestEndTime = this.isFinalEndTime;
    }
    this.resultEndTime = 0;

    this.lastAudioInput = [];
    this.lastAudioInput = this.audioInput;

    this.restartCounter++;

    if (!this.lastTranscriptWasFinal) {
      process.stdout.write("\n");
    }
    process.stdout.write(
      chalk.yellow(
        `${this.streamingLimit * this.restartCounter}: RESTARTING REQUEST\n`
      )
    );

    this.newStream = true;

    this.startRecognitionStream();
  }

  /**
   * Closes the recognize stream and wipes it
   */
  stopRecognitionStream() {
    if (this.recognizeStream) {
      this.recognizeStream.end();
			this.recognizeStream.removeAllListeners();
    }
    if (this.restartTimeout) {
      clearTimeout(this.restartTimeout);
    }
    this.recognizeStream = null;
  }
```

Google has a default timeout of five minutes for any streaming recognition API request. In order to perform transcriptions for longer than that limit, we use these two functions provided in the samples given by Google.

It is imperative that we remove all event listeners, otherwise Node will freak out.

Moving on to the front-end now.

---
### Setting up the Client side

Run these commands to set up the angular project

```bash
mkdir client && cd client
npx ng new speech-to-text --directory=.
npm install @google-cloud/speech socket.io-client
```

You can customize the `app.component` files as you like, but the basic template should have the socket connection as follows.

```ts
import { io, Socket } from 'socket.io-client';
import {AudioStreamer} from 'src/app/app.service';

constructor(private audioStreamer: AudioStreamer) {
  this.liveTranscription = '';
  this.socket = io('https://localhost:3000');
  this.socket.on('connect', () => {
    console.log('Socket Connection: ', this.socket?.connected);
  })
  this.audioStreamer._socket = this.socket;
}
startRecording() {
  console.log('startRecording');
  if (!this.isRecording) {
    this.liveTranscription = '';
  }
  this.isRecording = true;
  this.audioStreamer.initRecording((data: string) => {
    this.liveTranscription += ( data + '. ');
  }, (error) => {
    console.error('Error when processing audio');
  })
}
stopRecording() {
  this.isRecording = false;
  console.log('stopRecording');
  this.audioStreamer.stopRecording();
}
```

You can use the `liveTranscription` variable to output the transcription as html through Angular’s data binding.

```ts
import { Injectable } from '@angular/core';
import { Socket } from 'socket.io-client';

// Stream Audio
let bufferSize = 2048,
  AudioContext;

//audioStream constraints
const constraints = {
  audio: true,
  video: false,
};

@Injectable({
  providedIn: 'root',
})
export class AudioStreamer {
  input!: MediaStreamAudioSourceNode | null;
  globalStream: MediaStream | undefined;
  processor!: AudioWorkletNode | null;
  audioContext!: AudioContext | null;
  socket!: Socket;

  set _socket(value: Socket) {
    this.socket = value;
  }
  /**
   * @param {function} onData Callback to run on data each time it's received
   * @param {function} onError Callback to run on an error if one is emitted.
   */
  initRecording(onData: (arg0: any) => void, onError: (arg0: string) => void) {
    this.socket.emit('startGoogleCloudStream', {
      config: {
        encoding: 'LINEAR16',
        sampleRateHertz: 16000,
        languageCode: 'en-US',
        profanityFilter: false,
        enableWordTimeOffsets: true,
      },
      interimResults: true, // If you want interim results, set this to true
    }); //init socket Google Speech Connection
    AudioContext = window.AudioContext;
    this.audioContext = new AudioContext({
      latencyHint: 'interactive'
    });

    const handleSuccess = async (stream: MediaStream) => {
      this.globalStream = stream;
      this.input = this.audioContext!.createMediaStreamSource(stream);
      await this.audioContext!.audioWorklet.addModule(
        '/assets/audio-processor.js'
      );
      this.audioContext!.resume();
      this.processor = new AudioWorkletNode(
        this.audioContext!,
        'recorder.worklet'
      );
      this.processor.connect(this.audioContext!.destination);
      this.audioContext!.resume();
      this.input.connect(this.processor);

      this.processor.port.onmessage = (event: MessageEvent<ArrayBufferLike>) => {
        const audioData = event.data;
        this.sendAudio(audioData);
      }

    };

    navigator.mediaDevices.getUserMedia(constraints).then(handleSuccess);

    // Bind the data handler callback
    if (onData) {
      this.socket.on('speechData', (data) => {
        onData(data);
      });
    }

    this.socket.on('googleCloudStreamError', (error) => {
      if (onError) {
        onError('error');
      }
      // We don't want to emit another end stream event
      this.closeAll();
    });
  }
  
  sendAudio(buffer: ArrayBufferLike) {
    this.socket.emit('binaryAudioData', buffer);
  }

  stopRecording() {
    this.socket.emit('endGoogleCloudStream', '');
    this.closeAll();
  }

  /**
   * Stops recording and closes everything down. Runs on error or on stop.
   */
  closeAll() {
    // Clear the listeners (prevents issue if opening and closing repeatedly)
    this.socket.off('speechData');
    this.socket.off('googleCloudStreamError');
    let tracks = this.globalStream ? this.globalStream.getTracks() : null;
    let track = tracks ? tracks[0] : null;
    if (track) {
      track.stop();
    }

    if (this.processor) {
      if (this.input) {
        try {
          this.input.disconnect(this.processor);
        } catch (error) {
          console.warn('Attempt to disconnect input failed.');
        }
      }
      this.processor.disconnect(this.audioContext!.destination);
    }
    if (this.audioContext) {
      this.audioContext.close().then(() => {
        this.input = null;
        this.audioContext = null;
        AudioContext = null;
      });
    }
  }
}

export default AudioStreamer;
```

We use the [`AudioContext API`](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext) to process our microphone audio and send it to the backend via the socket connection. Google’s STT API expects audio in 16bit Integers, in order to convert this, we need to attach a custom audio processor that will resample our microphone input.

_**NOTE**:_ Make sure the `audio-processor.js` file is stored in assets folder otherwise Angular will not detect it.
Since the `AudioProcessor` is added as a worklet, it is detached from the main execution thread. So, we cannot send the data to the socket from the audio processor file. Instead, we make use of the inbuilt messaging function to send the down sampled bytes back to the service file.

Now we can send the audio to the socket.

Using an `EventListener` we can wait for the transcribed strings to be sent from the server and deal with them as we wish. Print them on the screen? Live subtitles for your videos? Anything is possible.

Even something like this: [Interview Warmup — Grow with Google](https://grow.google/certificates/interview-warmup/)

That concludes this article, thank you for reading this far ❤️

This article should give you a rundown on the detailed code provided here: [GitHub](https://github.com/rohitp934/cloud-speech)

